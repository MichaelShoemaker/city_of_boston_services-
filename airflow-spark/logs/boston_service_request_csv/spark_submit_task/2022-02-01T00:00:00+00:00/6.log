[2022-03-24 22:22:19,176] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: boston_service_request_csv.spark_submit_task 2022-02-01T00:00:00+00:00 [queued]>
[2022-03-24 22:22:19,218] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: boston_service_request_csv.spark_submit_task 2022-02-01T00:00:00+00:00 [queued]>
[2022-03-24 22:22:19,220] {taskinstance.py:1042} INFO - 
--------------------------------------------------------------------------------
[2022-03-24 22:22:19,221] {taskinstance.py:1043} INFO - Starting attempt 6 of 6
[2022-03-24 22:22:19,222] {taskinstance.py:1044} INFO - 
--------------------------------------------------------------------------------
[2022-03-24 22:22:19,250] {taskinstance.py:1063} INFO - Executing <Task(SparkSubmitOperator): spark_submit_task> on 2022-02-01T00:00:00+00:00
[2022-03-24 22:22:19,257] {standard_task_runner.py:52} INFO - Started process 966 to run task
[2022-03-24 22:22:19,268] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'boston_service_request_csv', 'spark_submit_task', '2022-02-01T00:00:00+00:00', '--job-id', '34', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/boston_service_request.py', '--cfg-path', '/tmp/tmpbzqynsur', '--error-file', '/tmp/tmpo59htv04']
[2022-03-24 22:22:19,274] {standard_task_runner.py:77} INFO - Job 34: Subtask spark_submit_task
[2022-03-24 22:22:19,377] {logging_mixin.py:104} INFO - Running <TaskInstance: boston_service_request_csv.spark_submit_task 2022-02-01T00:00:00+00:00 [running]> on host 8b234c07401a
[2022-03-24 22:22:19,494] {taskinstance.py:1257} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=boston_service_request_csv
AIRFLOW_CTX_TASK_ID=spark_submit_task
AIRFLOW_CTX_EXECUTION_DATE=2022-02-01T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-02-01T00:00:00+00:00
[2022-03-24 22:22:19,521] {base.py:74} INFO - Using connection to: id: spark_local. Host: local[*], Port: None, Schema: , Login: , Password: None, extra: None
[2022-03-24 22:22:19,530] {spark_submit.py:335} INFO - Spark-Submit cmd: spark-submit --master local[*] --name arrow-spark /home/airflow/datalake/project.py
[2022-03-24 22:22:26,610] {spark_submit.py:488} INFO - WARNING: An illegal reflective access operation has occurred
[2022-03-24 22:22:26,613] {spark_submit.py:488} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2022-03-24 22:22:26,615] {spark_submit.py:488} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2022-03-24 22:22:26,616] {spark_submit.py:488} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2022-03-24 22:22:26,618] {spark_submit.py:488} INFO - WARNING: All illegal access operations will be denied in a future release
[2022-03-24 22:42:20,410] {local_task_job.py:188} WARNING - State of this instance has been externally set to shutdown. Terminating instance.
[2022-03-24 22:42:20,454] {process_utils.py:100} INFO - Sending Signals.SIGTERM to GPID 966
[2022-03-24 22:42:20,462] {taskinstance.py:1239} ERROR - Received SIGTERM. Terminating subprocesses.
[2022-03-24 22:42:20,473] {spark_submit.py:623} INFO - Sending kill signal to spark-submit
[2022-03-24 22:42:20,518] {taskinstance.py:1455} ERROR - Task received SIGTERM signal
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1112, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1285, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1315, in _execute_task
    result = task_copy.execute(context=context)
  File "/usr/local/lib/python3.6/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/usr/local/lib/python3.6/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 407, in submit
    self._process_spark_submit_log(iter(self._submit_sp.stdout))  # type: ignore
  File "/usr/local/lib/python3.6/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 456, in _process_spark_submit_log
    for line in itr:
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1241, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
[2022-03-24 22:42:20,540] {taskinstance.py:1503} INFO - Marking task as FAILED. dag_id=boston_service_request_csv, task_id=spark_submit_task, execution_date=20220201T000000, start_date=20220324T222219, end_date=20220324T224220
[2022-03-24 22:42:20,756] {process_utils.py:66} INFO - Process psutil.Process(pid=968, status='terminated', started='22:22:19') (968) terminated with exit code None
[2022-03-24 22:42:20,760] {process_utils.py:66} INFO - Process psutil.Process(pid=966, status='terminated', exitcode=1, started='22:22:18') (966) terminated with exit code 1
[2022-03-26 04:51:17,227] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: boston_service_request_csv.spark_submit_task 2022-02-01T00:00:00+00:00 [queued]>
[2022-03-26 04:51:17,263] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: boston_service_request_csv.spark_submit_task 2022-02-01T00:00:00+00:00 [queued]>
[2022-03-26 04:51:17,265] {taskinstance.py:1042} INFO - 
--------------------------------------------------------------------------------
[2022-03-26 04:51:17,267] {taskinstance.py:1043} INFO - Starting attempt 6 of 6
[2022-03-26 04:51:17,269] {taskinstance.py:1044} INFO - 
--------------------------------------------------------------------------------
[2022-03-26 04:51:17,295] {taskinstance.py:1063} INFO - Executing <Task(SparkSubmitOperator): spark_submit_task> on 2022-02-01T00:00:00+00:00
[2022-03-26 04:51:17,304] {standard_task_runner.py:52} INFO - Started process 592 to run task
[2022-03-26 04:51:17,314] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'boston_service_request_csv', 'spark_submit_task', '2022-02-01T00:00:00+00:00', '--job-id', '56', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/boston_service_request.py', '--cfg-path', '/tmp/tmpqkuapz9n', '--error-file', '/tmp/tmporte0bln']
[2022-03-26 04:51:17,321] {standard_task_runner.py:77} INFO - Job 56: Subtask spark_submit_task
[2022-03-26 04:51:17,438] {logging_mixin.py:104} INFO - Running <TaskInstance: boston_service_request_csv.spark_submit_task 2022-02-01T00:00:00+00:00 [running]> on host 4c1f33c38df2
[2022-03-26 04:51:17,548] {taskinstance.py:1257} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=boston_service_request_csv
AIRFLOW_CTX_TASK_ID=spark_submit_task
AIRFLOW_CTX_EXECUTION_DATE=2022-02-01T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-02-01T00:00:00+00:00
[2022-03-26 04:51:17,574] {base.py:74} INFO - Using connection to: id: spark_local. Host: local[*], Port: None, Schema: , Login: , Password: None, extra: None
[2022-03-26 04:51:17,583] {spark_submit.py:335} INFO - Spark-Submit cmd: spark-submit --master local[*] --name arrow-spark /home/airflow/datalake/project.py
[2022-03-26 04:51:24,168] {spark_submit.py:488} INFO - WARNING: An illegal reflective access operation has occurred
[2022-03-26 04:51:24,170] {spark_submit.py:488} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2022-03-26 04:51:24,172] {spark_submit.py:488} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2022-03-26 04:51:24,174] {spark_submit.py:488} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2022-03-26 04:51:24,177] {spark_submit.py:488} INFO - WARNING: All illegal access operations will be denied in a future release
[2022-03-26 04:51:35,754] {spark_submit.py:488} INFO - 2022-03-26 04:51:35,750 INFO spark.SparkContext: Running Spark version 3.0.0
[2022-03-26 04:51:35,936] {spark_submit.py:488} INFO - 2022-03-26 04:51:35,935 INFO resource.ResourceUtils: ==============================================================
[2022-03-26 04:51:35,941] {spark_submit.py:488} INFO - 2022-03-26 04:51:35,941 INFO resource.ResourceUtils: Resources for spark.driver:
[2022-03-26 04:51:35,943] {spark_submit.py:488} INFO - 
[2022-03-26 04:51:35,945] {spark_submit.py:488} INFO - 2022-03-26 04:51:35,943 INFO resource.ResourceUtils: ==============================================================
[2022-03-26 04:51:35,946] {spark_submit.py:488} INFO - 2022-03-26 04:51:35,945 INFO spark.SparkContext: Submitted application: Boston Service Request
[2022-03-26 04:51:36,202] {spark_submit.py:488} INFO - 2022-03-26 04:51:36,201 INFO spark.SecurityManager: Changing view acls to: airflow
[2022-03-26 04:51:36,204] {spark_submit.py:488} INFO - 2022-03-26 04:51:36,203 INFO spark.SecurityManager: Changing modify acls to: airflow
[2022-03-26 04:51:36,207] {spark_submit.py:488} INFO - 2022-03-26 04:51:36,204 INFO spark.SecurityManager: Changing view acls groups to:
[2022-03-26 04:51:36,208] {spark_submit.py:488} INFO - 2022-03-26 04:51:36,204 INFO spark.SecurityManager: Changing modify acls groups to:
[2022-03-26 04:51:36,210] {spark_submit.py:488} INFO - 2022-03-26 04:51:36,204 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(airflow); groups with view permissions: Set(); users  with modify permissions: Set(airflow); groups with modify permissions: Set()
[2022-03-26 04:51:37,430] {spark_submit.py:488} INFO - 2022-03-26 04:51:37,429 INFO util.Utils: Successfully started service 'sparkDriver' on port 39771.
[2022-03-26 04:51:37,569] {spark_submit.py:488} INFO - 2022-03-26 04:51:37,568 INFO spark.SparkEnv: Registering MapOutputTracker
[2022-03-26 04:51:37,715] {spark_submit.py:488} INFO - 2022-03-26 04:51:37,715 INFO spark.SparkEnv: Registering BlockManagerMaster
[2022-03-26 04:51:37,781] {spark_submit.py:488} INFO - 2022-03-26 04:51:37,780 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-03-26 04:51:37,785] {spark_submit.py:488} INFO - 2022-03-26 04:51:37,784 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-03-26 04:51:37,809] {spark_submit.py:488} INFO - 2022-03-26 04:51:37,808 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-03-26 04:51:37,854] {spark_submit.py:488} INFO - 2022-03-26 04:51:37,854 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-6b73b6b4-b608-47a8-a780-50f20139e834
[2022-03-26 04:51:37,943] {spark_submit.py:488} INFO - 2022-03-26 04:51:37,942 INFO memory.MemoryStore: MemoryStore started with capacity 434.4 MiB
[2022-03-26 04:51:38,012] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,012 INFO spark.SparkEnv: Registering OutputCommitCoordinator
[2022-03-26 04:51:38,375] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,374 INFO util.log: Logging initialized @19358ms to org.sparkproject.jetty.util.log.Slf4jLog
[2022-03-26 04:51:38,598] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,597 INFO server.Server: jetty-9.4.z-SNAPSHOT; built: 2019-04-29T20:42:08.989Z; git: e1bc35120a6617ee3df052294e433f3a25ce7097; jvm 11.0.14+9-post-Debian-1deb10u1
[2022-03-26 04:51:38,666] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,665 INFO server.Server: Started @19656ms
[2022-03-26 04:51:38,762] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,761 INFO server.AbstractConnector: Started ServerConnector@3011f37e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[2022-03-26 04:51:38,764] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,762 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
[2022-03-26 04:51:38,856] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,856 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1f93dbb2{/jobs,null,AVAILABLE,@Spark}
[2022-03-26 04:51:38,865] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,865 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ff41dbb{/jobs/json,null,AVAILABLE,@Spark}
[2022-03-26 04:51:38,868] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,868 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2fafd870{/jobs/job,null,AVAILABLE,@Spark}
[2022-03-26 04:51:38,884] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,883 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@23e3a2f5{/jobs/job/json,null,AVAILABLE,@Spark}
[2022-03-26 04:51:38,887] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,887 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ae813d7{/stages,null,AVAILABLE,@Spark}
[2022-03-26 04:51:38,890] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,890 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27e354f9{/stages/json,null,AVAILABLE,@Spark}
[2022-03-26 04:51:38,893] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,893 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ee83221{/stages/stage,null,AVAILABLE,@Spark}
[2022-03-26 04:51:38,907] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,906 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28098f91{/stages/stage/json,null,AVAILABLE,@Spark}
[2022-03-26 04:51:38,911] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,911 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@45de0aad{/stages/pool,null,AVAILABLE,@Spark}
[2022-03-26 04:51:38,915] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,915 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@17dca8a5{/stages/pool/json,null,AVAILABLE,@Spark}
[2022-03-26 04:51:38,921] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,921 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4ed4bd11{/storage,null,AVAILABLE,@Spark}
[2022-03-26 04:51:38,926] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,925 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21cd4c54{/storage/json,null,AVAILABLE,@Spark}
[2022-03-26 04:51:38,932] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,932 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6304b4a3{/storage/rdd,null,AVAILABLE,@Spark}
[2022-03-26 04:51:38,937] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,937 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@495fd24e{/storage/rdd/json,null,AVAILABLE,@Spark}
[2022-03-26 04:51:38,942] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,942 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a9aae3c{/environment,null,AVAILABLE,@Spark}
[2022-03-26 04:51:38,946] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,946 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7388f5a5{/environment/json,null,AVAILABLE,@Spark}
[2022-03-26 04:51:38,951] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,950 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@da5b682{/executors,null,AVAILABLE,@Spark}
[2022-03-26 04:51:38,954] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,954 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69801a11{/executors/json,null,AVAILABLE,@Spark}
[2022-03-26 04:51:38,958] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,958 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a763dc{/executors/threadDump,null,AVAILABLE,@Spark}
[2022-03-26 04:51:38,966] {spark_submit.py:488} INFO - 2022-03-26 04:51:38,966 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3813d564{/executors/threadDump/json,null,AVAILABLE,@Spark}
[2022-03-26 04:51:39,007] {spark_submit.py:488} INFO - 2022-03-26 04:51:39,006 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1595ef91{/static,null,AVAILABLE,@Spark}
[2022-03-26 04:51:39,012] {spark_submit.py:488} INFO - 2022-03-26 04:51:39,011 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@529889e8{/,null,AVAILABLE,@Spark}
[2022-03-26 04:51:39,020] {spark_submit.py:488} INFO - 2022-03-26 04:51:39,020 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@306bd6e5{/api,null,AVAILABLE,@Spark}
[2022-03-26 04:51:39,028] {spark_submit.py:488} INFO - 2022-03-26 04:51:39,027 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f8b41b2{/jobs/job/kill,null,AVAILABLE,@Spark}
[2022-03-26 04:51:39,038] {spark_submit.py:488} INFO - 2022-03-26 04:51:39,038 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d0d8eb4{/stages/stage/kill,null,AVAILABLE,@Spark}
[2022-03-26 04:51:39,052] {spark_submit.py:488} INFO - 2022-03-26 04:51:39,052 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://4c1f33c38df2:4040
[2022-03-26 04:51:40,064] {spark_submit.py:488} INFO - 2022-03-26 04:51:40,063 INFO executor.Executor: Starting executor ID driver on host 4c1f33c38df2
[2022-03-26 04:51:40,186] {spark_submit.py:488} INFO - 2022-03-26 04:51:40,185 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46329.
[2022-03-26 04:51:40,189] {spark_submit.py:488} INFO - 2022-03-26 04:51:40,189 INFO netty.NettyBlockTransferService: Server created on 4c1f33c38df2:46329
[2022-03-26 04:51:40,198] {spark_submit.py:488} INFO - 2022-03-26 04:51:40,197 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-03-26 04:51:40,233] {spark_submit.py:488} INFO - 2022-03-26 04:51:40,233 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4c1f33c38df2, 46329, None)
[2022-03-26 04:51:40,263] {spark_submit.py:488} INFO - 2022-03-26 04:51:40,259 INFO storage.BlockManagerMasterEndpoint: Registering block manager 4c1f33c38df2:46329 with 434.4 MiB RAM, BlockManagerId(driver, 4c1f33c38df2, 46329, None)
[2022-03-26 04:51:40,278] {spark_submit.py:488} INFO - 2022-03-26 04:51:40,277 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4c1f33c38df2, 46329, None)
[2022-03-26 04:51:40,287] {spark_submit.py:488} INFO - 2022-03-26 04:51:40,286 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 4c1f33c38df2, 46329, None)
[2022-03-26 04:51:40,955] {spark_submit.py:488} INFO - 2022-03-26 04:51:40,954 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2267d569{/metrics/json,null,AVAILABLE,@Spark}
[2022-03-26 04:51:42,944] {spark_submit.py:488} INFO - 2022-03-26 04:51:42,943 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/airflow/spark-warehouse').
[2022-03-26 04:51:42,946] {spark_submit.py:488} INFO - 2022-03-26 04:51:42,946 INFO internal.SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.
[2022-03-26 04:51:43,010] {spark_submit.py:488} INFO - 2022-03-26 04:51:43,010 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9f27fc8{/SQL,null,AVAILABLE,@Spark}
[2022-03-26 04:51:43,014] {spark_submit.py:488} INFO - 2022-03-26 04:51:43,014 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3572f1ca{/SQL/json,null,AVAILABLE,@Spark}
[2022-03-26 04:51:43,019] {spark_submit.py:488} INFO - 2022-03-26 04:51:43,018 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7d24ee49{/SQL/execution,null,AVAILABLE,@Spark}
[2022-03-26 04:51:43,026] {spark_submit.py:488} INFO - 2022-03-26 04:51:43,025 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3d7c150c{/SQL/execution/json,null,AVAILABLE,@Spark}
[2022-03-26 04:51:43,101] {spark_submit.py:488} INFO - 2022-03-26 04:51:43,100 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c5e0b1b{/static/sql,null,AVAILABLE,@Spark}
[2022-03-26 06:28:05,206] {local_task_job.py:188} WARNING - State of this instance has been externally set to failed. Terminating instance.
[2022-03-26 06:28:05,234] {process_utils.py:100} INFO - Sending Signals.SIGTERM to GPID 592
[2022-03-26 06:28:05,239] {taskinstance.py:1239} ERROR - Received SIGTERM. Terminating subprocesses.
[2022-03-26 06:28:05,256] {spark_submit.py:623} INFO - Sending kill signal to spark-submit
[2022-03-26 06:28:05,929] {process_utils.py:66} INFO - Process psutil.Process(pid=594, status='terminated', started='04:51:17') (594) terminated with exit code None
[2022-03-26 06:28:05,934] {process_utils.py:66} INFO - Process psutil.Process(pid=592, status='terminated', exitcode=0, started='04:51:16') (592) terminated with exit code 0
[2022-03-26 06:28:05,937] {process_utils.py:66} INFO - Process psutil.Process(pid=661, status='terminated', started='04:51:26') (661) terminated with exit code None
