[2022-03-24 20:28:53,864] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: boston_service_request_csv.spark_submit_task 2022-02-01T00:00:00+00:00 [queued]>
[2022-03-24 20:28:53,899] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: boston_service_request_csv.spark_submit_task 2022-02-01T00:00:00+00:00 [queued]>
[2022-03-24 20:28:53,902] {taskinstance.py:1042} INFO - 
--------------------------------------------------------------------------------
[2022-03-24 20:28:53,903] {taskinstance.py:1043} INFO - Starting attempt 5 of 5
[2022-03-24 20:28:53,904] {taskinstance.py:1044} INFO - 
--------------------------------------------------------------------------------
[2022-03-24 20:28:53,930] {taskinstance.py:1063} INFO - Executing <Task(SparkSubmitOperator): spark_submit_task> on 2022-02-01T00:00:00+00:00
[2022-03-24 20:28:53,939] {standard_task_runner.py:52} INFO - Started process 1279 to run task
[2022-03-24 20:28:53,950] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'boston_service_request_csv', 'spark_submit_task', '2022-02-01T00:00:00+00:00', '--job-id', '31', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/boston_service_request.py', '--cfg-path', '/tmp/tmpp5vr0ggg', '--error-file', '/tmp/tmpgw4ie7p4']
[2022-03-24 20:28:53,956] {standard_task_runner.py:77} INFO - Job 31: Subtask spark_submit_task
[2022-03-24 20:28:54,053] {logging_mixin.py:104} INFO - Running <TaskInstance: boston_service_request_csv.spark_submit_task 2022-02-01T00:00:00+00:00 [running]> on host 947848e5da77
[2022-03-24 20:28:54,166] {taskinstance.py:1257} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=boston_service_request_csv
AIRFLOW_CTX_TASK_ID=spark_submit_task
AIRFLOW_CTX_EXECUTION_DATE=2022-02-01T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-02-01T00:00:00+00:00
[2022-03-24 20:28:54,191] {base.py:74} INFO - Using connection to: id: spark_local. Host: local[*], Port: None, Schema: , Login: , Password: None, extra: None
[2022-03-24 20:28:54,199] {spark_submit.py:335} INFO - Spark-Submit cmd: spark-submit --master local[*] --name arrow-spark /home/airflow/datalake/project.py
[2022-03-24 20:29:00,783] {spark_submit.py:488} INFO - WARNING: An illegal reflective access operation has occurred
[2022-03-24 20:29:00,786] {spark_submit.py:488} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2022-03-24 20:29:00,788] {spark_submit.py:488} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2022-03-24 20:29:00,789] {spark_submit.py:488} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2022-03-24 20:29:00,791] {spark_submit.py:488} INFO - WARNING: All illegal access operations will be denied in a future release
[2022-03-24 20:29:10,630] {spark_submit.py:488} INFO - 2022-03-24 20:29:10,616 INFO spark.SparkContext: Running Spark version 3.0.0
[2022-03-24 20:29:10,970] {spark_submit.py:488} INFO - 2022-03-24 20:29:10,969 INFO resource.ResourceUtils: ==============================================================
[2022-03-24 20:29:10,978] {spark_submit.py:488} INFO - 2022-03-24 20:29:10,977 INFO resource.ResourceUtils: Resources for spark.driver:
[2022-03-24 20:29:10,979] {spark_submit.py:488} INFO - 
[2022-03-24 20:29:10,981] {spark_submit.py:488} INFO - 2022-03-24 20:29:10,980 INFO resource.ResourceUtils: ==============================================================
[2022-03-24 20:29:10,984] {spark_submit.py:488} INFO - 2022-03-24 20:29:10,983 INFO spark.SparkContext: Submitted application: Boston Service Request
[2022-03-24 20:29:11,262] {spark_submit.py:488} INFO - 2022-03-24 20:29:11,261 INFO spark.SecurityManager: Changing view acls to: airflow
[2022-03-24 20:29:11,264] {spark_submit.py:488} INFO - 2022-03-24 20:29:11,262 INFO spark.SecurityManager: Changing modify acls to: airflow
[2022-03-24 20:29:11,265] {spark_submit.py:488} INFO - 2022-03-24 20:29:11,263 INFO spark.SecurityManager: Changing view acls groups to:
[2022-03-24 20:29:11,268] {spark_submit.py:488} INFO - 2022-03-24 20:29:11,263 INFO spark.SecurityManager: Changing modify acls groups to:
[2022-03-24 20:29:11,270] {spark_submit.py:488} INFO - 2022-03-24 20:29:11,264 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(airflow); groups with view permissions: Set(); users  with modify permissions: Set(airflow); groups with modify permissions: Set()
[2022-03-24 20:29:12,596] {spark_submit.py:488} INFO - 2022-03-24 20:29:12,594 INFO util.Utils: Successfully started service 'sparkDriver' on port 42263.
[2022-03-24 20:29:12,745] {spark_submit.py:488} INFO - 2022-03-24 20:29:12,745 INFO spark.SparkEnv: Registering MapOutputTracker
[2022-03-24 20:29:12,895] {spark_submit.py:488} INFO - 2022-03-24 20:29:12,894 INFO spark.SparkEnv: Registering BlockManagerMaster
[2022-03-24 20:29:12,965] {spark_submit.py:488} INFO - 2022-03-24 20:29:12,964 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-03-24 20:29:12,969] {spark_submit.py:488} INFO - 2022-03-24 20:29:12,968 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-03-24 20:29:12,993] {spark_submit.py:488} INFO - 2022-03-24 20:29:12,993 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-03-24 20:29:13,040] {spark_submit.py:488} INFO - 2022-03-24 20:29:13,039 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-7d3453c4-986b-4e45-8831-b4a73eb5fb8a
[2022-03-24 20:29:13,130] {spark_submit.py:488} INFO - 2022-03-24 20:29:13,129 INFO memory.MemoryStore: MemoryStore started with capacity 434.4 MiB
[2022-03-24 20:29:13,204] {spark_submit.py:488} INFO - 2022-03-24 20:29:13,203 INFO spark.SparkEnv: Registering OutputCommitCoordinator
[2022-03-24 20:29:13,552] {spark_submit.py:488} INFO - 2022-03-24 20:29:13,551 INFO util.log: Logging initialized @17903ms to org.sparkproject.jetty.util.log.Slf4jLog
[2022-03-24 20:29:13,753] {spark_submit.py:488} INFO - 2022-03-24 20:29:13,752 INFO server.Server: jetty-9.4.z-SNAPSHOT; built: 2019-04-29T20:42:08.989Z; git: e1bc35120a6617ee3df052294e433f3a25ce7097; jvm 11.0.14+9-post-Debian-1deb10u1
[2022-03-24 20:29:13,851] {spark_submit.py:488} INFO - 2022-03-24 20:29:13,850 INFO server.Server: Started @18209ms
[2022-03-24 20:29:13,945] {spark_submit.py:488} INFO - 2022-03-24 20:29:13,944 INFO server.AbstractConnector: Started ServerConnector@4758bca0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[2022-03-24 20:29:13,947] {spark_submit.py:488} INFO - 2022-03-24 20:29:13,945 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
[2022-03-24 20:29:14,050] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,049 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1818e0e0{/jobs,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,059] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,059 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@697d6b0a{/jobs/json,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,063] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,062 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7af959f2{/jobs/job,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,079] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,078 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f958f2f{/jobs/job/json,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,082] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,081 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d7e3c8e{/stages,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,084] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,084 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@45a438dc{/stages/json,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,088] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,087 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48e21d8f{/stages/stage,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,102] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,102 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7800ef47{/stages/stage/json,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,106] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,106 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@13136ab1{/stages/pool,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,110] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,109 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a527aea{/stages/pool/json,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,115] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,114 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f56711{/storage,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,120] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,120 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@544be1ac{/storage/json,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,125] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,125 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29541c22{/storage/rdd,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,129] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,128 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25773b33{/storage/rdd/json,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,134] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,133 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9283e95{/environment,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,141] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,141 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c17e9cc{/environment/json,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,146] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,145 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@8ed128{/executors,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,149] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,149 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@c2cd7ea{/executors/json,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,154] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,153 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@66ca8b3b{/executors/threadDump,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,163] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,161 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6318822c{/executors/threadDump/json,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,210] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,210 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@19b62be7{/static,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,216] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,215 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e17569d{/,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,223] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,223 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@657173ca{/api,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,232] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,232 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@602ede68{/jobs/job/kill,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,243] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,242 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d80b657{/stages/stage/kill,null,AVAILABLE,@Spark}
[2022-03-24 20:29:14,254] {spark_submit.py:488} INFO - 2022-03-24 20:29:14,253 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://947848e5da77:4040
[2022-03-24 20:29:15,145] {spark_submit.py:488} INFO - 2022-03-24 20:29:15,145 INFO executor.Executor: Starting executor ID driver on host 947848e5da77
[2022-03-24 20:29:15,271] {spark_submit.py:488} INFO - 2022-03-24 20:29:15,271 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34707.
[2022-03-24 20:29:15,274] {spark_submit.py:488} INFO - 2022-03-24 20:29:15,274 INFO netty.NettyBlockTransferService: Server created on 947848e5da77:34707
[2022-03-24 20:29:15,284] {spark_submit.py:488} INFO - 2022-03-24 20:29:15,283 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-03-24 20:29:15,318] {spark_submit.py:488} INFO - 2022-03-24 20:29:15,317 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 947848e5da77, 34707, None)
[2022-03-24 20:29:15,346] {spark_submit.py:488} INFO - 2022-03-24 20:29:15,342 INFO storage.BlockManagerMasterEndpoint: Registering block manager 947848e5da77:34707 with 434.4 MiB RAM, BlockManagerId(driver, 947848e5da77, 34707, None)
[2022-03-24 20:29:15,366] {spark_submit.py:488} INFO - 2022-03-24 20:29:15,366 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 947848e5da77, 34707, None)
[2022-03-24 20:29:15,373] {spark_submit.py:488} INFO - 2022-03-24 20:29:15,373 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 947848e5da77, 34707, None)
[2022-03-24 20:29:16,046] {spark_submit.py:488} INFO - 2022-03-24 20:29:16,044 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48372e03{/metrics/json,null,AVAILABLE,@Spark}
[2022-03-24 20:29:17,256] {spark_submit.py:488} INFO - 2022-03-24 20:29:17,255 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/airflow/spark-warehouse').
[2022-03-24 20:29:17,259] {spark_submit.py:488} INFO - 2022-03-24 20:29:17,257 INFO internal.SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.
[2022-03-24 20:29:17,308] {spark_submit.py:488} INFO - 2022-03-24 20:29:17,307 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58bf63df{/SQL,null,AVAILABLE,@Spark}
[2022-03-24 20:29:17,310] {spark_submit.py:488} INFO - 2022-03-24 20:29:17,310 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@468ce5f5{/SQL/json,null,AVAILABLE,@Spark}
[2022-03-24 20:29:17,315] {spark_submit.py:488} INFO - 2022-03-24 20:29:17,315 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4d29cd29{/SQL/execution,null,AVAILABLE,@Spark}
[2022-03-24 20:29:17,320] {spark_submit.py:488} INFO - 2022-03-24 20:29:17,320 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e1034ec{/SQL/execution/json,null,AVAILABLE,@Spark}
[2022-03-24 20:29:17,392] {spark_submit.py:488} INFO - 2022-03-24 20:29:17,391 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63d6999d{/static/sql,null,AVAILABLE,@Spark}
[2022-03-24 20:29:19,869] {spark_submit.py:488} INFO - Traceback (most recent call last):
[2022-03-24 20:29:19,870] {spark_submit.py:488} INFO - File "/home/airflow/datalake/project.py", line 75, in <module>
[2022-03-24 20:29:19,872] {spark_submit.py:488} INFO - df = spark.read.option("header",True).schema(schema).csv("gs://datalake-311-bronze/boston_2022.csv")
[2022-03-24 20:29:19,873] {spark_submit.py:488} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 535, in csv
[2022-03-24 20:29:19,877] {spark_submit.py:488} INFO - File "/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__
[2022-03-24 20:29:19,880] {spark_submit.py:488} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 131, in deco
[2022-03-24 20:29:19,882] {spark_submit.py:488} INFO - File "/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 328, in get_return_value
[2022-03-24 20:29:19,887] {spark_submit.py:488} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o35.csv.
[2022-03-24 20:29:19,888] {spark_submit.py:488} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2022-03-24 20:29:19,889] {spark_submit.py:488} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
[2022-03-24 20:29:19,891] {spark_submit.py:488} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
[2022-03-24 20:29:19,892] {spark_submit.py:488} INFO - at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
[2022-03-24 20:29:19,894] {spark_submit.py:488} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
[2022-03-24 20:29:19,895] {spark_submit.py:488} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
[2022-03-24 20:29:19,896] {spark_submit.py:488} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
[2022-03-24 20:29:19,897] {spark_submit.py:488} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2022-03-24 20:29:19,898] {spark_submit.py:488} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)
[2022-03-24 20:29:19,899] {spark_submit.py:488} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)
[2022-03-24 20:29:19,900] {spark_submit.py:488} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)
[2022-03-24 20:29:19,901] {spark_submit.py:488} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)
[2022-03-24 20:29:19,902] {spark_submit.py:488} INFO - at scala.Option.getOrElse(Option.scala:189)
[2022-03-24 20:29:19,903] {spark_submit.py:488} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)
[2022-03-24 20:29:19,904] {spark_submit.py:488} INFO - at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:705)
[2022-03-24 20:29:19,906] {spark_submit.py:488} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2022-03-24 20:29:19,906] {spark_submit.py:488} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2022-03-24 20:29:19,907] {spark_submit.py:488} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2022-03-24 20:29:19,908] {spark_submit.py:488} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2022-03-24 20:29:19,909] {spark_submit.py:488} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2022-03-24 20:29:19,910] {spark_submit.py:488} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2022-03-24 20:29:19,911] {spark_submit.py:488} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2022-03-24 20:29:19,912] {spark_submit.py:488} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2022-03-24 20:29:19,913] {spark_submit.py:488} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2022-03-24 20:29:19,914] {spark_submit.py:488} INFO - at py4j.GatewayConnection.run(GatewayConnection.java:238)
[2022-03-24 20:29:19,915] {spark_submit.py:488} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-03-24 20:29:19,916] {spark_submit.py:488} INFO - 
[2022-03-24 20:30:20,227] {spark_submit.py:488} INFO - 2022-03-24 20:30:20,220 ERROR util.ShutdownHookManager: ShutdownHookManger shutdown forcefully after 30 seconds.
[2022-03-24 20:30:20,828] {taskinstance.py:1455} ERROR - Cannot execute: spark-submit --master local[*] --name arrow-spark /home/airflow/datalake/project.py. Error code is: 1.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1112, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1285, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1315, in _execute_task
    result = task_copy.execute(context=context)
  File "/usr/local/lib/python3.6/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/usr/local/lib/python3.6/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 420, in submit
    f"Cannot execute: {self._mask_cmd(spark_submit_cmd)}. Error code is: {returncode}."
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local[*] --name arrow-spark /home/airflow/datalake/project.py. Error code is: 1.
[2022-03-24 20:30:20,847] {taskinstance.py:1503} INFO - Marking task as FAILED. dag_id=boston_service_request_csv, task_id=spark_submit_task, execution_date=20220201T000000, start_date=20220324T202853, end_date=20220324T203020
[2022-03-24 20:30:20,928] {local_task_job.py:146} INFO - Task exited with return code 1
[2022-03-25 04:03:34,420] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: boston_service_request_csv.spark_submit_task 2022-02-01T00:00:00+00:00 [queued]>
[2022-03-25 04:03:34,463] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: boston_service_request_csv.spark_submit_task 2022-02-01T00:00:00+00:00 [queued]>
[2022-03-25 04:03:34,465] {taskinstance.py:1042} INFO - 
--------------------------------------------------------------------------------
[2022-03-25 04:03:34,466] {taskinstance.py:1043} INFO - Starting attempt 5 of 5
[2022-03-25 04:03:34,468] {taskinstance.py:1044} INFO - 
--------------------------------------------------------------------------------
[2022-03-25 04:03:34,497] {taskinstance.py:1063} INFO - Executing <Task(SparkSubmitOperator): spark_submit_task> on 2022-02-01T00:00:00+00:00
[2022-03-25 04:03:34,505] {standard_task_runner.py:52} INFO - Started process 574 to run task
[2022-03-25 04:03:34,515] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'boston_service_request_csv', 'spark_submit_task', '2022-02-01T00:00:00+00:00', '--job-id', '49', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/boston_service_request.py', '--cfg-path', '/tmp/tmpdcgl0h0c', '--error-file', '/tmp/tmpmtlif1zx']
[2022-03-25 04:03:34,522] {standard_task_runner.py:77} INFO - Job 49: Subtask spark_submit_task
[2022-03-25 04:03:34,617] {logging_mixin.py:104} INFO - Running <TaskInstance: boston_service_request_csv.spark_submit_task 2022-02-01T00:00:00+00:00 [running]> on host 8f00cae6f9bc
[2022-03-25 04:03:34,716] {taskinstance.py:1257} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=boston_service_request_csv
AIRFLOW_CTX_TASK_ID=spark_submit_task
AIRFLOW_CTX_EXECUTION_DATE=2022-02-01T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-02-01T00:00:00+00:00
[2022-03-25 04:03:34,742] {base.py:74} INFO - Using connection to: id: spark_local. Host: local[*], Port: None, Schema: , Login: , Password: None, extra: None
[2022-03-25 04:03:34,752] {spark_submit.py:335} INFO - Spark-Submit cmd: spark-submit --master local[*] --name arrow-spark /home/airflow/datalake/project.py
[2022-03-25 04:03:41,559] {spark_submit.py:488} INFO - WARNING: An illegal reflective access operation has occurred
[2022-03-25 04:03:41,561] {spark_submit.py:488} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2022-03-25 04:03:41,563] {spark_submit.py:488} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2022-03-25 04:03:41,565] {spark_submit.py:488} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2022-03-25 04:03:41,567] {spark_submit.py:488} INFO - WARNING: All illegal access operations will be denied in a future release
[2022-03-25 04:03:51,973] {spark_submit.py:488} INFO - 2022-03-25 04:03:51,962 INFO spark.SparkContext: Running Spark version 3.0.0
[2022-03-25 04:03:52,301] {spark_submit.py:488} INFO - 2022-03-25 04:03:52,300 INFO resource.ResourceUtils: ==============================================================
[2022-03-25 04:03:52,308] {spark_submit.py:488} INFO - 2022-03-25 04:03:52,307 INFO resource.ResourceUtils: Resources for spark.driver:
[2022-03-25 04:03:52,310] {spark_submit.py:488} INFO - 
[2022-03-25 04:03:52,312] {spark_submit.py:488} INFO - 2022-03-25 04:03:52,310 INFO resource.ResourceUtils: ==============================================================
[2022-03-25 04:03:52,315] {spark_submit.py:488} INFO - 2022-03-25 04:03:52,313 INFO spark.SparkContext: Submitted application: Boston Service Request
[2022-03-25 04:03:52,674] {spark_submit.py:488} INFO - 2022-03-25 04:03:52,673 INFO spark.SecurityManager: Changing view acls to: airflow
[2022-03-25 04:03:52,677] {spark_submit.py:488} INFO - 2022-03-25 04:03:52,675 INFO spark.SecurityManager: Changing modify acls to: airflow
[2022-03-25 04:03:52,680] {spark_submit.py:488} INFO - 2022-03-25 04:03:52,676 INFO spark.SecurityManager: Changing view acls groups to:
[2022-03-25 04:03:52,683] {spark_submit.py:488} INFO - 2022-03-25 04:03:52,676 INFO spark.SecurityManager: Changing modify acls groups to:
[2022-03-25 04:03:52,685] {spark_submit.py:488} INFO - 2022-03-25 04:03:52,677 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(airflow); groups with view permissions: Set(); users  with modify permissions: Set(airflow); groups with modify permissions: Set()
[2022-03-25 04:03:53,941] {spark_submit.py:488} INFO - 2022-03-25 04:03:53,938 INFO util.Utils: Successfully started service 'sparkDriver' on port 37847.
[2022-03-25 04:03:54,090] {spark_submit.py:488} INFO - 2022-03-25 04:03:54,090 INFO spark.SparkEnv: Registering MapOutputTracker
[2022-03-25 04:03:54,235] {spark_submit.py:488} INFO - 2022-03-25 04:03:54,234 INFO spark.SparkEnv: Registering BlockManagerMaster
[2022-03-25 04:03:54,292] {spark_submit.py:488} INFO - 2022-03-25 04:03:54,291 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-03-25 04:03:54,295] {spark_submit.py:488} INFO - 2022-03-25 04:03:54,295 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-03-25 04:03:54,320] {spark_submit.py:488} INFO - 2022-03-25 04:03:54,320 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-03-25 04:03:54,372] {spark_submit.py:488} INFO - 2022-03-25 04:03:54,371 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-c16d2679-4cb7-488b-9a58-51184fc7367c
[2022-03-25 04:03:54,462] {spark_submit.py:488} INFO - 2022-03-25 04:03:54,461 INFO memory.MemoryStore: MemoryStore started with capacity 434.4 MiB
[2022-03-25 04:03:54,531] {spark_submit.py:488} INFO - 2022-03-25 04:03:54,530 INFO spark.SparkEnv: Registering OutputCommitCoordinator
[2022-03-25 04:03:54,947] {spark_submit.py:488} INFO - 2022-03-25 04:03:54,947 INFO util.log: Logging initialized @18752ms to org.sparkproject.jetty.util.log.Slf4jLog
[2022-03-25 04:03:55,174] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,173 INFO server.Server: jetty-9.4.z-SNAPSHOT; built: 2019-04-29T20:42:08.989Z; git: e1bc35120a6617ee3df052294e433f3a25ce7097; jvm 11.0.14+9-post-Debian-1deb10u1
[2022-03-25 04:03:55,245] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,244 INFO server.Server: Started @19058ms
[2022-03-25 04:03:55,345] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,344 INFO server.AbstractConnector: Started ServerConnector@270ae048{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
[2022-03-25 04:03:55,348] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,345 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
[2022-03-25 04:03:55,444] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,443 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10d236ab{/jobs,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,452] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,452 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5585c84d{/jobs/json,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,456] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,456 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ebd3c19{/jobs/job,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,472] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,472 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7453c8a2{/jobs/job/json,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,476] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,475 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4fdccd2a{/stages,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,479] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,478 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3db5e4de{/stages/json,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,482] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,481 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29033f9a{/stages/stage,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,500] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,500 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@73d06f5a{/stages/stage/json,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,505] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,504 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3b0947eb{/stages/pool,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,509] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,508 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@799575ed{/stages/pool/json,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,513] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,513 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6dc5a0e8{/storage,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,518] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,518 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4d992606{/storage/json,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,524] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,523 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@545b9e3{/storage/rdd,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,529] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,528 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@169e3747{/storage/rdd/json,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,533] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,533 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@47184579{/environment,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,536] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,536 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2e361eff{/environment/json,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,541] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,541 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@17981047{/executors,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,549] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,548 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6936cead{/executors/json,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,555] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,554 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@59d525dd{/executors/threadDump,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,563] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,563 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ea2bbad{/executors/threadDump/json,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,607] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,607 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4ca19e{/static,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,614] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,614 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@8d01e62{/,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,622] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,621 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@73e36052{/api,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,628] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,628 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e5fe091{/jobs/job/kill,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,636] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,635 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74fdb68f{/stages/stage/kill,null,AVAILABLE,@Spark}
[2022-03-25 04:03:55,653] {spark_submit.py:488} INFO - 2022-03-25 04:03:55,653 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://8f00cae6f9bc:4040
[2022-03-25 04:03:56,674] {spark_submit.py:488} INFO - 2022-03-25 04:03:56,673 INFO executor.Executor: Starting executor ID driver on host 8f00cae6f9bc
[2022-03-25 04:03:56,767] {spark_submit.py:488} INFO - 2022-03-25 04:03:56,766 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41841.
[2022-03-25 04:03:56,769] {spark_submit.py:488} INFO - 2022-03-25 04:03:56,769 INFO netty.NettyBlockTransferService: Server created on 8f00cae6f9bc:41841
[2022-03-25 04:03:56,778] {spark_submit.py:488} INFO - 2022-03-25 04:03:56,777 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-03-25 04:03:56,801] {spark_submit.py:488} INFO - 2022-03-25 04:03:56,801 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8f00cae6f9bc, 41841, None)
[2022-03-25 04:03:56,819] {spark_submit.py:488} INFO - 2022-03-25 04:03:56,816 INFO storage.BlockManagerMasterEndpoint: Registering block manager 8f00cae6f9bc:41841 with 434.4 MiB RAM, BlockManagerId(driver, 8f00cae6f9bc, 41841, None)
[2022-03-25 04:03:56,831] {spark_submit.py:488} INFO - 2022-03-25 04:03:56,830 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8f00cae6f9bc, 41841, None)
[2022-03-25 04:03:56,836] {spark_submit.py:488} INFO - 2022-03-25 04:03:56,836 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 8f00cae6f9bc, 41841, None)
[2022-03-25 04:03:57,496] {spark_submit.py:488} INFO - 2022-03-25 04:03:57,494 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ad1ef37{/metrics/json,null,AVAILABLE,@Spark}
[2022-03-25 04:03:59,471] {spark_submit.py:488} INFO - 2022-03-25 04:03:59,470 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/airflow/spark-warehouse').
[2022-03-25 04:03:59,474] {spark_submit.py:488} INFO - 2022-03-25 04:03:59,474 INFO internal.SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.
[2022-03-25 04:03:59,539] {spark_submit.py:488} INFO - 2022-03-25 04:03:59,538 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2231c831{/SQL,null,AVAILABLE,@Spark}
[2022-03-25 04:03:59,543] {spark_submit.py:488} INFO - 2022-03-25 04:03:59,543 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e1bc0c4{/SQL/json,null,AVAILABLE,@Spark}
[2022-03-25 04:03:59,551] {spark_submit.py:488} INFO - 2022-03-25 04:03:59,550 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4da8852{/SQL/execution,null,AVAILABLE,@Spark}
[2022-03-25 04:03:59,558] {spark_submit.py:488} INFO - 2022-03-25 04:03:59,556 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7d8386a9{/SQL/execution/json,null,AVAILABLE,@Spark}
[2022-03-25 04:03:59,624] {spark_submit.py:488} INFO - 2022-03-25 04:03:59,624 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3be85b8f{/static/sql,null,AVAILABLE,@Spark}
[2022-03-25 04:08:45,594] {taskinstance.py:1166} INFO - Marking task as SUCCESS. dag_id=boston_service_request_csv, task_id=spark_submit_task, execution_date=20220201T000000, start_date=20220325T040334, end_date=20220325T040845
[2022-03-25 04:08:45,658] {taskinstance.py:1220} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2022-03-25 04:08:45,712] {local_task_job.py:146} INFO - Task exited with return code 0
